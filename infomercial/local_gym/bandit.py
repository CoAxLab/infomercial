#! /usr/bin/env python
import numpy as np
import gym
from gym import spaces
from gym.utils import seeding
from itertools import cycle

# Gym is annoying these days...
import warnings
warnings.filterwarnings("ignore")


class ExampleInfoBandit1a(gym.Env):
    """Illustration of info regularity (y,y,y,...,b,b,b)"""
    def __init__(self):
        self.num_arms = 1
        self.best = [0]
        self.stim = (1, 2)
        self.action_space = spaces.Discrete(1)
        self.observation_space = spaces.Discrete(2)
        self.reset()
        self.states = [1] * 20 + [2] * 20
        self.max_steps = len(self.states)
        self.count = 0

    def step(self, action):
        if self.count > self.max_steps:
            raise ValueError(f"env exceeded max_steps ({self.count})")

        state = self.states[self.count]
        reward = 0

        self.count += 1

        return state, reward, self.done, {}

    def reset(self):
        self.done = False
        return 0

    def render(self, mode='human', close=False):
        pass


class ExampleInfoBandit1b(gym.Env):
    """Example of changes in regularity (y,b,y,b,...,b,b,b)"""
    def __init__(self):
        self.num_arms = 1
        self.best = [0]
        self.stim = (1, 2)
        self.action_space = spaces.Discrete(1)
        self.observation_space = spaces.Discrete(2)
        self.reset()
        self.states = [1, 2] * 10 + [2] * 20
        self.max_steps = len(self.states)
        self.count = 0

    def step(self, action):
        if self.count > self.max_steps:
            raise ValueError(f"env exceeded max_steps ({self.count})")

        state = self.states[self.count]
        reward = 0

        self.count += 1

        return state, reward, self.done, {}

    def reset(self):
        self.done = False
        return 0

    def render(self, mode='human', close=False):
        pass


class ExampleInfoBandit1c(gym.Env):
    """Example of changes in regularity (binomal(y,b),...,b,b,b)"""
    def __init__(self):
        self.num_arms = 1
        self.best = [0]
        self.stim = (1, 2)
        self.action_space = spaces.Discrete(1)
        self.observation_space = spaces.Discrete(2)
        self.reset()
        # First part of the list generated by
        # np.random.binomial(1, size=20, p=.5) + 1
        # Second hald if perfect regularity
        self.states = [
            1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 1
        ] + [2] * 20
        self.max_steps = len(self.states)
        self.count = 0

    def step(self, action):
        if self.count > self.max_steps:
            raise ValueError(f"env exceeded max_steps ({self.count})")

        state = self.states[self.count]
        reward = 0

        self.count += 1

        return state, reward, self.done, {}

    def reset(self):
        self.done = False
        return 0

    def render(self, mode='human', close=False):
        pass


class ExampleBandit4(gym.Env):
    """Example to illustrate BanditOneHigh4, in a consistent way."""
    def __init__(self):
        self.num_arms = 4
        self.best = [1]

        self.action_space = spaces.Discrete(4)
        self.observation_space = spaces.Discrete(1)
        self.reset()
        self.count = 0

        # Build pre-defined rewards.
        self.rewards = {}

        # --------------------------------------------------------------------
        # NOTE: All the sequences below were generated using np.random.binomial
        # and only arm 1 was modified to have an 'unnatural' run of zeros
        # in the middle. This make a nice example dataset for explore/
        # exploit policies to fight it out.
        # --------------------------------------------------------------------

        # - Worst arms
        # arm: 0, 2, 3 p(reward) ~= 0.2
        self.rewards[0] = [
            0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
            0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0
        ]
        self.rewards[2] = [
            1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0
        ]
        self.rewards[3] = [
            1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,
            1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
            0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,
            1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0
        ]

        # - Best arm
        # arm: 1 p(reward) ~= 0.8 ...BUT it is modified to have 'doubts'
        self.rewards[1] = [
            1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,
            1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,
            1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,
            0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0
        ]
        self.max_steps = len(self.rewards[1])

    def step(self, action):
        if not self.action_space.contains(action):
            raise ValueError("action not known")

        state = 0
        reward = self.rewards[action][self.count % self.max_steps]

        self.count += 1

        return state, reward, self.done, {}

    def seed(self, seed):
        pass

    def reset(self):
        self.done = False
        return 0

    def render(self, mode='human', close=False):
        pass


class DistractionBanditEnv(gym.Env):
    """
    n-armed info bandit environment. Rewards are independent of
    states, whose information is a distraction.

    Params
    ------
    stim : list 
        A list of possible return states    
    s_dists : list of tuples 
        A list of prob of a stim, for each arm
    r_dists : list of tuples 
        A list of prob of reward (0,1), for each arm
    """
    def __init__(self, stim, rewards, s_dists, r_dists):

        # check for sizes and p_norm
        for i, s_dist in enumerate(s_dists):
            if len(s_dist) != len(stim):
                raise ValueError(f"Entry {i} in s_dists is the wrong len")
            if not np.isclose(np.sum(s_dist), 1):
                raise ValueError(f"Entry {i} in s_dists does not sum to 1")

        # check for sizes and p_norm
        for i, r_dist in enumerate(r_dists):
            if len(r_dist) != len(rewards):
                raise ValueError(f"Entry {i} in r_dists is the wrong len")
            if not np.isclose(np.sum(r_dist), 1):
                raise ValueError(f"Entry {i} in r_dists does not sum to 1")

        self.stim = stim
        self.n_stim = len(self.stim)
        self.s_dists = s_dists
        self.n_bandits = len(s_dists)

        self.rewards = rewards
        self.r_dists = r_dists

        self.action_space = spaces.Discrete(self.n_bandits)
        self.observation_space = spaces.Discrete(self.n_stim)
        self.seed()
        self.reset()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        assert self.action_space.contains(action)
        if self.done:
            raise ValueError("Cannot step, env is done.")

        self.done = True
        p_dist = self.s_dists[action]
        state = self.np_random.choice(self.stim, p=p_dist)

        r_dist = self.r_dists[action]
        reward = self.np_random.choice(self.rewards, p=r_dist)

        return state, reward, self.done, {}

    def reset(self):
        self.done = False
        return [0]

    def render(self, mode='human', close=False):
        pass


class DistractionBanditOneHigh10(DistractionBanditEnv):
    """A (0.8, 0.2, 0.2, ...) bandit, with distracting stim/states."""
    def __init__(self):
        self.num_arms = 10
        self.best = [7]  # for reward

        # Stim/state
        stim = [1, 2]
        s_dists = [(0.5, 0.5)] * self.num_arms

        # Reward
        rewards = [0, 1]
        r_dists = [(0.8, 0.2)] * self.num_arms
        r_dists[self.best[0]] = (0.2, 0.8)

        DistractionBanditEnv.__init__(self,
                                      stim=stim,
                                      rewards=rewards,
                                      s_dists=s_dists,
                                      r_dists=r_dists)


class InfoBanditEnv(gym.Env):
    """
    n-armed info bandit environment. Rewards are alwats zero, but the return
    states can offer information, abstractly.

    Params
    ------
    stim : list 
        A list of possible return states    
    p_dists : list of tuples 
        A list of prob of stim, for each bandit
    """
    def __init__(self, stim, p_dists):

        # check for sizes and p_norm
        for i, p_dist in enumerate(p_dists):
            if len(p_dist) != len(stim):
                raise ValueError(f"Entry {i} in p_dists is the wrong len")
            if not np.isclose(np.sum(p_dist), 1):
                raise ValueError(f"Entry {i} in p_dists does not sum to 1")

        self.n_stim = len(stim)
        self.stim = stim
        self.p_dists = p_dists
        self.n_bandits = len(p_dists)

        self.action_space = spaces.Discrete(self.n_bandits)
        self.observation_space = spaces.Discrete(self.n_stim)
        self.seed()
        self.reset()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        assert self.action_space.contains(action)
        if self.done:
            raise ValueError("Cannot step, env is done.")

        reward = 0
        self.done = True

        p_dist = self.p_dists[action]
        state = self.np_random.choice(self.stim, p=p_dist)

        return state, reward, self.done, {}

    def reset(self):
        self.done = False
        return [0]

    def render(self, mode='human', close=False):
        pass


class InfoBlueYellow2a(InfoBanditEnv):
    """A blue/yellow info bandit with one nearly certain arm,
     and one max entropy arm.
     
    Stimuli code:
    ----
    Blue : 1
    Yellow : 2
    """
    def __init__(self):
        self.best = [1]
        self.num_arms = 2
        stim = [1, 2]
        p_dists = [(0.99, 0.01), (0.5, 0.5)]
        InfoBanditEnv.__init__(self, stim=stim, p_dists=p_dists)


class InfoBlueYellow2b(InfoBanditEnv):
    """A blue/yellow info bandit, with two max entropy arms.
    
    Stimuli code:
    ----
    Blue : 1
    Yellow : 2
    """
    def __init__(self):
        self.best = [0, 1]
        self.num_arms = 2
        stim = [1, 2]
        p_dists = [(0.5, 0.5), (0.5, 0.5)]
        InfoBanditEnv.__init__(self, stim=stim, p_dists=p_dists)


class InfoBlueYellow4a(InfoBanditEnv):
    """A blue/yellow info bandit, with one max entropy arm.
    
    Stimuli code:
    ----
    Blue : 1
    Yellow : 2
    """
    def __init__(self):
        self.best = [1]
        self.num_arms = 4
        stim = [1, 2]
        p_dists = [(0.99, 0.01), (0.5, 0.5), (0.99, 0.01), (0.01, 0.99)]
        InfoBanditEnv.__init__(self, stim=stim, p_dists=p_dists)


class InfoBlueYellow4b(InfoBanditEnv):
    """A blue/yellow info bandit, with max entropy arms

    Stimuli code:
    ----
    Blue : 1
    Yellow : 2
    """
    def __init__(self):
        self.best = [0, 1, 2, 3]
        self.num_arms = 4
        stim = [1, 2]
        p_dists = [(0.5, 0.5), (0.5, 0.5), (0.5, 0.5), (0.5, 0.5)]
        InfoBanditEnv.__init__(self, stim=stim, p_dists=p_dists)


class InfoBlueYellow4c(InfoBanditEnv):
    """A blue/yellow info bandit, with a variety of arm probs.

    Stimuli code:
    ----
    Blue : 1
    Yellow : 2
    """
    def __init__(self):
        self.best = [1]
        self.num_arms = 4
        stim = [1, 2]
        p_dists = [(0.6, 0.4), (0.5, 0.5), (0.9, 0.1), (0.2, 0.8)]
        InfoBanditEnv.__init__(self, stim=stim, p_dists=p_dists)


class BanditEnv(gym.Env):
    """
    n-armed bandit environment  

    Params
    ------
    p_dist : list
        A list of probabilities of the likelihood that a particular bandit will pay out
    r_dist : list or list or lists
        A list of either rewards (if number) or means and standard deviations (if list) of the payout that bandit has
    """
    def __init__(self, p_dist, r_dist):
        if len(p_dist) != len(r_dist):
            raise ValueError(
                "Probability and Reward distribution must be the same length")

        if min(p_dist) < 0 or max(p_dist) > 1:
            raise ValueError("All probabilities must be between 0 and 1")

        for reward in r_dist:
            if isinstance(reward, list) and reward[1] <= 0:
                raise ValueError(
                    "Standard deviation in rewards must all be greater than 0")

        self.p_dist = p_dist
        self.r_dist = r_dist

        self.n_bandits = len(p_dist)
        self.action_space = spaces.Discrete(self.n_bandits)
        self.observation_space = spaces.Discrete(1)
        self.seed()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        assert self.action_space.contains(action)
        if self.done:
            raise ValueError("Cannot step, env is done.")

        state = 0
        reward = 0
        self.done = True

        if self.np_random.uniform() < self.p_dist[action]:
            if not isinstance(self.r_dist[action], list):
                reward = self.r_dist[action]
            else:
                reward = self.np_random.normal(self.r_dist[action][0],
                                               self.r_dist[action][1])

        return state, reward, self.done, {}

    def reset(self):
        self.done = False
        return [0]

    def render(self, mode='human', close=False):
        pass


class UnstableBanditEnv(gym.Env):
    """n-armed bandit, but the winning probabilites are unstable."""
    def __init__(self, p_dists, r_dists, unstable_rate):
        for p_dist, r_dist in zip(p_dists, r_dists):
            if len(p_dist) != len(r_dist):
                raise ValueError(
                    "Probability and Reward distribution must be the same length"
                )

            if min(p_dist) < 0 or max(p_dist) > 1:
                raise ValueError("All probabilities must be between 0 and 1")

            for reward in r_dist:
                if isinstance(reward, list) and reward[1] <= 0:
                    raise ValueError(
                        "Standard deviation in rewards must all be greater than 0"
                    )

        # Set up first dist
        self.p_dists = p_dists
        self.r_dists = r_dists
        self._random_bandit()

        self.unstable_rate = unstable_rate

        # Setup the space
        self.n_bandits = len(self.p_dist)
        self.action_space = spaces.Discrete(self.n_bandits)
        self.observation_space = spaces.Discrete(1)
        self.seed()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def _random_bandit(self):
        i = self.np_random.randint(0, len(self.p_dists))
        self.p_dist = self.p_dists[i]
        self.r_dist = self.r_dists[i]

    def step(self, action):
        assert self.action_space.contains(action)
        if self.done:
            raise ValueError("Cannot step, env is done.")

        state = 0
        reward = 0
        self.done = True

        if self.np_random.uniform() < self.p_dist[action]:
            if not isinstance(self.r_dist[action], list):
                reward = self.r_dist[action]
            else:
                reward = self.np_random.normal(self.r_dist[action][0],
                                               self.r_dist[action][1])

        # Change bandit?
        if self.np_random.poisson(self.unstable_rate) > 0:
            self._random_bandit()

        return state, reward, self.done, {}

    def reset(self):
        self.done = False
        return [0]

    def render(self, mode='human', close=False):
        pass


class ChangeBanditEnv(gym.Env):
    """
    n-armed bandit environment, where the best arm changes half the time.

    Params
    ------
    p_dist : list
        A list of probabilities of the likelihood that a particular bandit will pay out
    """
    def __init__(self, p_dist1, p_dist2, min_steps=10, max_steps=100):
        if min_steps < 1:
            raise ValueError('min_steps must be > 1')
        if min_steps > max_steps:
            raise ValueError('min_steps must be less than max_steps')
        if len(p_dist1) != len(p_dist2):
            raise ValueError('p_dists dont match')
        if min(p_dist1) < 0 or max(p_dist1) > 1:
            raise ValueError("All probabilities must be between 0 and 1")
        if min(p_dist2) < 0 or max(p_dist2) > 1:
            raise ValueError("All probabilities must be between 0 and 1")

        # Init
        self.min_steps = min_steps
        self.max_steps = max_steps

        self.p_dist1 = p_dist1  # first dist
        self.p_dist2 = p_dist2  # second dist, if we change
        self.p_dist = self.p_dist1
        self.best = np.argmax(self.p_dist)

        self.n_bandits = len(self.p_dist)
        self.action_space = spaces.Discrete(self.n_bandits)
        self.observation_space = spaces.Discrete(1)
        self.steps = 0
        self.seed()

        # Do change, and where?
        self.change_episode = 0
        if self.np_random.rand() > 0.5:
            self.change_episode = self.np_random.randint(
                self.min_steps, self.max_steps)

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        # Sanity
        if self.steps > self.max_steps:
            raise EnvironmentError("Number of steps exceeded max.")
        assert self.action_space.contains(action)

        # Change?
        if self.steps == self.change_episode:
            self.p_dist = self.p_dist2
            self.best = np.argmax(self.p_dist)

        # Build return
        state = 0
        reward = 0
        self.done = True
        if self.np_random.rand() < self.p_dist[action]:
            reward = 1

        # -
        self.steps += 1
        return state, reward, self.done, {}

    def reset(self):
        self.done = False
        return 0

    def render(self, mode='human', close=False):
        pass


class DeceptiveBanditEnv(gym.Env):
    """
    n-armed bandit environment, you have to move steps_away to find the best arm.

    Params
    ------
    p_dist : list
        A list of probabilities of the likelihood that a particular bandit will pay out
    r_dist : list or list or lists
        A list of either rewards (if number) or means and standard deviations (if list) of the payout that bandit has
    """
    def __init__(self, p_dist, r_dist, steps_away=1, max_steps=30):
        if len(p_dist) != len(r_dist):
            raise ValueError(
                "Probability and Reward distribution must be the same length")

        if min(p_dist) < 0 or max(p_dist) > 1:
            raise ValueError("All probabilities must be between 0 and 1")

        for reward in r_dist:
            if isinstance(reward, list) and reward[1] <= 0:
                raise ValueError(
                    "Standard deviation in rewards must all be greater than 0")

        if max_steps < (2 * steps_away):
            raise ValueError("max_steps must be greater than 2*steps_away")
        self.p_dist = p_dist
        self.r_dist = r_dist
        self.steps = 0
        self.max_steps = max_steps
        self.steps_away = steps_away
        self.scale = np.concatenate(
            (np.linspace(-1, 0, steps_away), np.linspace(0, 1, steps_away)))

        self.n_bandits = len(p_dist)
        self.action_space = spaces.Discrete(self.n_bandits)
        self.observation_space = spaces.Discrete(1)

        self.seed()

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        return [seed]

    def step(self, action):
        # Sanity
        if self.steps > self.max_steps:
            raise EnvironmentError("Number of steps exceeded max.")

        # Action is in the space?

        action = int(action)
        assert self.action_space.contains(action)

        # Get the reward....
        self.done = True

        reward = 0
        if self.np_random.uniform() < self.p_dist[action]:
            reward = self.r_dist[action]

        # Add deceptiveness. Only the best arms are deceptive.
        if (action in self.best) and (reward != 0):
            try:
                reward *= self.scale[self.steps]
            except IndexError:
                reward *= np.max(self.scale)

            self.steps += 1

        return 0, float(reward), self.done, {}

    def reset(self):
        self.done = False
        return [0]

    def render(self, mode='human', close=False):
        pass


class DeceptiveBanditOneHigh10(DeceptiveBanditEnv):
    """A (0.8, 0.2, 0.2, ...) bandit."""
    def __init__(self):
        self.best = [7]
        self.num_arms = 10

        # Set p(R > 0)
        p_dist = [0.2] * self.num_arms
        p_dist[self.best[0]] = 0.8

        # Set baseline R
        r_dist = [1] * self.num_arms

        DeceptiveBanditEnv.__init__(self,
                                    p_dist=p_dist,
                                    r_dist=r_dist,
                                    steps_away=10,
                                    max_steps=500)


class BanditOneHot2(BanditEnv):
    """A one winner bandit."""
    def __init__(self):
        self.best = [0]
        self.num_arms = 2

        p_dist = [0] * self.num_arms
        p_dist[self.best[0]] = 1
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditOneHot10(BanditEnv):
    """A one winner bandit."""
    def __init__(self):
        self.best = [7]
        self.num_arms = 10

        p_dist = [0] * self.num_arms
        p_dist[self.best[0]] = 1
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditOneHot121(BanditEnv):
    """A one winner bandit."""
    def __init__(self):
        self.best = [54]
        self.num_arms = 121

        p_dist = [0] * self.num_arms
        p_dist[self.best[0]] = 1
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditOneHot1000(BanditEnv):
    """A one winner bandit."""
    def __init__(self):
        self.best = [526]
        self.num_arms = 1000

        p_dist = [0] * self.num_arms
        p_dist[self.best[0]] = 1
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditEvenOdds2(BanditEnv):
    """A 50/50 bandit."""
    def __init__(self):
        BanditEnv.__init__(self, p_dist=[0.5, 0.5], r_dist=[1, 1])


class BanditOneHigh2(BanditEnv):
    """A (0.8, 0.2) bandit."""
    def __init__(self):
        self.best = [0]
        self.num_arms = 2

        p_dist = [0.2] * self.num_arms
        p_dist[self.best[0]] = 0.8
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditOneHigh4(BanditEnv):
    """A (0.2, 0.8, 0.2, 0.2) bandit."""
    def __init__(self):
        self.best = [1]
        self.num_arms = 4

        p_dist = [0.2] * self.num_arms
        p_dist[self.best[0]] = 0.8
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditOneHigh10(BanditEnv):
    """A (0.8, 0.2, 0.2, ...) bandit."""
    def __init__(self):
        self.best = [7]
        self.num_arms = 10

        p_dist = [0.2] * self.num_arms
        p_dist[self.best[0]] = 0.8
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditOneHigh121(BanditEnv):
    """A (0.8, 0.2, 0.2, ...) bandit."""
    def __init__(self):
        self.best = [54]
        self.num_arms = 121

        p_dist = [0.2] * self.num_arms
        p_dist[self.best[0]] = 0.8
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditTwoHigh10(BanditEnv):
    """A (..., 0.60, ..., 0.80. 0.2, 0.2, ...) bandit."""
    def __init__(self):
        self.best = [7]
        self.num_arms = 10

        p_dist = [0.2] * self.num_arms
        p_dist[3] = 0.60
        p_dist[self.best[0]] = 0.80
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditTwoHigh121(BanditEnv):
    """A (..., 0.80, ..., 0.80. 0.2, 0.2, ...) bandit."""
    def __init__(self):
        self.best = [71]
        self.num_arms = 121

        p_dist = [0.2] * self.num_arms
        p_dist[26] = 0.60
        p_dist[self.best[0]] = 0.80
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditOneHigh1000(BanditEnv):
    """A (0.8, 0.2, 0.2, ...) bandit."""
    def __init__(self):
        self.best = [526]
        self.num_arms = 1000

        p_dist = [0.2] * self.num_arms
        p_dist[self.best[0]] = 0.8
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditTwoHigh1000(BanditEnv):
    """A (..., 0.80, ..., 0.80. 0.2, 0.2, ...) bandit."""
    def __init__(self):
        self.best = [731]
        self.num_arms = 1000

        p_dist = [0.2] * self.num_arms
        p_dist[526] = 0.60
        p_dist[self.best[0]] = 0.80
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditTwoExtreme1000(BanditEnv):
    """A (..., 0.99, ..., 0.99. 0.01, 0.01, ...) bandit."""
    def __init__(self):
        self.best = [526, 731]
        self.num_arms = 1000

        p_dist = [0.01] * self.num_arms
        p_dist[self.best[0]] = 0.99
        p_dist[self.best[1]] = 0.99
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditHardAndSparse2(BanditEnv):
    """A (0.10,0.08,0.08,....) bandit"""
    def __init__(self):
        self.best = [0]
        self.num_arms = 2

        p_dist = [0.01] * self.num_arms
        p_dist[self.best[0]] = 0.02
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditHardAndSparse10(BanditEnv):
    """A (0.10,0.08,0.08,....) bandit"""
    def __init__(self):
        self.best = [7]
        self.num_arms = 10

        p_dist = [0.01] * self.num_arms
        p_dist[self.best[0]] = 0.02
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditHardAndSparse121(BanditEnv):
    """A (0.10,0.08,0.08,....) bandit"""
    def __init__(self):
        self.best = [54]
        self.num_arms = 121

        p_dist = [0.01] * self.num_arms
        p_dist[self.best[0]] = 0.02
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditHardAndSparse1000(BanditEnv):
    """A (0.10,0.08,0.08,....) bandit"""
    def __init__(self):
        self.best = [526]
        self.num_arms = 1000

        p_dist = [0.01] * self.num_arms
        p_dist[self.best[0]] = 0.02
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)


class BanditUniform10(BanditEnv):
    """A U(0.2, 0.75) bandit, with one best set 0.8."""
    def __init__(self):
        self.best = [7]
        self.num_arms = 10

        p_dist = np.random.uniform(0.2, 0.6, size=self.num_arms).tolist()
        p_dist[self.best[0]] = 0.8
        r_dist = [1] * self.num_arms
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)

        # Reset p(R) dist with the seed
        self.p_dist = self.np_random.uniform(0.2, 0.6,
                                             size=self.num_arms).tolist()
        self.p_dist[self.best[0]] = 0.8

        return [seed]


class BanditUniform121(BanditEnv):
    """A U(0.2, 0.6) bandit, with one best set 0.8."""
    def __init__(self):
        self.best = [54]
        self.num_arms = 121

        # ---
        self.p_min = 0.2
        self.p_max = 0.6
        self.p_best = 0.8

        # Generate intial p_dist
        # (gets overwritten is seed())
        p_dist = np.random.uniform(self.p_min, self.p_max,
                                   size=self.num_arms).tolist()
        p_dist[self.best[0]] = self.p_best

        # reward
        r_dist = [1] * self.num_arms

        # ---
        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)

        # Reset p(R) dist with the seed
        self.p_dist = self.np_random.uniform(self.p_min,
                                             self.p_max,
                                             size=self.num_arms).tolist()
        self.p_dist[self.best[0]] = self.p_best

        return [seed]


class BanditGaussian10(BanditEnv):
    """
    10 armed bandit mentioned on page 30 of Sutton and Barto's
    [Reinforcement Learning: An Introduction](https://www.dropbox.com/s/b3psxv2r0ccmf80/book2015oct.pdf?dl=0)

    Actions always pay out.
    Mean of payout is pulled from a normal distribution (0, 1) (called q*(a))
    Actual reward is drawn from a normal distribution (q*(a), 1)
    """
    def __init__(self, bandits=10):
        p_dist = np.full(bandits, 1)
        r_dist = []

        for i in range(bandits):
            r_dist.append([self.np_random.normal(0, 1), 1])

        BanditEnv.__init__(self, p_dist=p_dist, r_dist=r_dist)