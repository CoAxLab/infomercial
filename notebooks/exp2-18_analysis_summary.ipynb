{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 2-18 analysis summary.\n",
    "\n",
    "Which parameters did best on the tasks?\n",
    "\n",
    "See `./informercial/Makefile` for experimental\n",
    "details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('ticks')\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "matplotlib.rc('axes', titlesize=16)\n",
    "\n",
    "from infomercial.exp import info_bandit\n",
    "from infomercial.local_gym import bandit\n",
    "from infomercial.exp.info_bandit import load_checkpoint\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ls ../data/exp2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path =\"/Users/qualia/Code/infomercial/data/\"\n",
    "exp_names = [\"exp2\",\n",
    "             \"exp3\",\n",
    "             \"exp4\",\n",
    "             \"exp5\",\n",
    "             \"exp6\",\n",
    "             \"exp7\",\n",
    "             \"exp8\",\n",
    "             \"exp9\",\n",
    "             \"exp10\",\n",
    "             \"exp11\",\n",
    "             \"exp12\",\n",
    "             \"exp13\",\n",
    "             \"exp14\",\n",
    "             \"exp15\",\n",
    "             \"exp16\",\n",
    "             \"exp17\",             \n",
    "             \"exp18\"]\n",
    "exp_index = list(range(2, 19))\n",
    "\n",
    "num_exps = 50\n",
    "num_episodes = 10000\n",
    "env_names = [\n",
    "    \"BanditOneHigh2-v0\",\n",
    "    \"BanditOneHigh10-v0\",\n",
    "    \"BanditOneHigh121-v0\",\n",
    "    \"BanditOneHigh1000-v0\",\n",
    "    \"BanditHardAndSparse2-v0\",\n",
    "    \"BanditHardAndSparse10-v0\",\n",
    "    \"BanditHardAndSparse121-v0\", \n",
    "    \"BanditHardAndSparse1000-v0\"\n",
    "]\n",
    "\n",
    "last_trials = -500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each exp, then each task, extract the p_best, and the last_trials.\n",
    "\n",
    "# Init final result\n",
    "p_best = {}\n",
    "for env in env_names:\n",
    "        p_best[env] = np.zeros(len(exp_names))\n",
    "        \n",
    "for j, exp_name in enumerate(exp_names):\n",
    "    # Gather traces by bandit: scores, \n",
    "    # Qs in a big numpy array (n_exp, n_episodes)\n",
    "    scores_E = {}\n",
    "    scores_R = {}\n",
    "    values_E = {}\n",
    "    values_R = {}\n",
    "    controlling = {}\n",
    "    actions = {}\n",
    "    best = {}\n",
    "\n",
    "    # Preallocate the arrays for this env\n",
    "    for env in env_names:\n",
    "        scores_E[env] = np.zeros((num_episodes, num_exps))\n",
    "        scores_R[env] = np.zeros((num_episodes, num_exps))\n",
    "        values_E[env] = np.zeros((num_episodes, num_exps))\n",
    "        values_R[env] = np.zeros((num_episodes, num_exps))\n",
    "        controlling[env] = np.zeros((num_episodes, num_exps))\n",
    "        actions[env] = np.zeros((num_episodes, num_exps))\n",
    "        best[env] = None\n",
    "\n",
    "        # Load and repackage\n",
    "        for n in range(num_exps):\n",
    "            result = load_checkpoint(os.path.join(data_path, f\"{exp_name}_{env}_{n+1}.pkl\"))\n",
    "            scores_E[env][:, n] = result[\"scores_E\"]\n",
    "            scores_R[env][:, n] = result[\"scores_R\"]\n",
    "            values_E[env][:, n] = result[\"values_E\"]\n",
    "            values_R[env][:, n] = result[\"values_R\"]\n",
    "            controlling[env][:, n] = result[\"policies\"]\n",
    "            actions[env][:, n] = result[\"actions\"]\n",
    "            best[env] = result[\"best\"]\n",
    "\n",
    "    # Est. prob. that the action was correct.\n",
    "    p_best_e = {}\n",
    "    for env in env_names:\n",
    "        b = best[env]\n",
    "        p_best_e[env] = np.zeros(num_episodes)\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            actions_i = actions[env][i,:]\n",
    "            p_best_e[env][i] = np.sum(actions_i == b) / actions_i.size\n",
    "            \n",
    "    # Get avg. p_best of last_trials for each exp and env\n",
    "    for env in env_names:\n",
    "        p_best[env][j] = np.mean(p_best_e[env][last_trials:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning performance\n",
    "\n",
    "For each bandit separatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sum_performance(plot_names):\n",
    "    fig = plt.figure(figsize=(4, 3*len(plot_names)))\n",
    "    grid = plt.GridSpec(len(plot_names), 1, wspace=0.4, hspace=1.2)\n",
    "\n",
    "    for i, env in enumerate(plot_names):\n",
    "        plt.subplot(grid[i, 0])\n",
    "        plt.title(f\"{env}\")\n",
    "        b = best[env]\n",
    "        for n in range(num_exps):\n",
    "            ps = p_best[env]\n",
    "            plt.scatter(exp_index, ps, color=\"black\", alpha=1, s=16)\n",
    "            plt.plot(exp_index, np.ones(len(exp_index)), color=\"grey\", alpha=0.2, ls='--', linewidth=1)\n",
    "            plt.ylabel(\"p(best)\")\n",
    "            plt.xlabel(\"Experiment number\")\n",
    "            plt.xticks(exp_index)\n",
    "            plt.ylim(-.1, 1.1)\n",
    "    _ = sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare exps\n",
    "\n",
    "### OneHigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_sum_performance(env_names[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sum_performance(env_names[4:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
