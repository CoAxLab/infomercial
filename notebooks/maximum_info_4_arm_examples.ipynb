{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from infomercial.util import Distribution\n",
    "from infomercial.discrete.value import information_value\n",
    "from infomercial.local_gym import BanditFourArmedDeterministicFixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a 4 armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_arms = 4\n",
    "env = BanditFourArmedDeterministicFixed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, beta=1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x*beta) / np.sum(np.exp(x*beta), axis=0)\n",
    "\n",
    "def policy(x, beta=1):\n",
    "    \"\"\"Softmax action selection policy\"\"\"\n",
    "    p = softmax(x, beta=beta)\n",
    "    actions = np.arange(0, len(x))\n",
    "    return np.random.choice(actions, p=p), p\n",
    "    \n",
    "def learn(agent, delta, lr=0.1):\n",
    "    agent += lr * delta\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Action 1, p [0.00726541 0.81519239 0.16996908 0.00757312] -> Avg. R 1.0\n",
      "1000 Action 1, p [0.00251383 0.99245818 0.00251417 0.00251383] -> Avg. R 0.98001998001998\n",
      "2000 Action 1, p [0.00266565 0.99200306 0.00266565 0.00266565] -> Avg. R 0.9855072463768116\n",
      "0 Action 1, p [0.00726541 0.81519239 0.16996908 0.00757312] -> Avg. R 1.0\n",
      "1000 Action 1, p [0.00251383 0.99245818 0.00251417 0.00251383] -> Avg. R 0.98001998001998\n",
      "2000 Action 1, p [0.00266565 0.99200306 0.00266565 0.00266565] -> Avg. R 0.9855072463768116\n",
      "3000 Action 1, p [0.00247417 0.99257748 0.00247417 0.00247417] -> Avg. R 0.9893368877040987\n",
      "4000 Action 1, p [0.00268625 0.99194126 0.00268625 0.00268625] -> Avg. R 0.9900024993751562\n",
      "5000 Action 1, p [0.00254745 0.99235766 0.00254745 0.00254745] -> Avg. R 0.9894021195760848\n",
      "6000 Action 1, p [0.00246083 0.9926175  0.00246083 0.00246083] -> Avg. R 0.9903349441759707\n",
      "3000 Action 1, p [0.00247417 0.99257748 0.00247417 0.00247417] -> Avg. R 0.9893368877040987\n",
      "4000 Action 1, p [0.00268625 0.99194126 0.00268625 0.00268625] -> Avg. R 0.9900024993751562\n",
      "5000 Action 1, p [0.00254745 0.99235766 0.00254745 0.00254745] -> Avg. R 0.9894021195760848\n",
      "6000 Action 1, p [0.00246083 0.9926175  0.00246083 0.00246083] -> Avg. R 0.9903349441759707\n",
      "7000 Action 1, p [0.00246081 0.99261756 0.00246081 0.00246081] -> Avg. R 0.9914297957434652\n",
      "8000 Action 1, p [0.00249357 0.99251928 0.00249357 0.00249357] -> Avg. R 0.991626046744157\n",
      "9000 Action 1, p [0.00253288 0.99240135 0.00253288 0.00253288] -> Avg. R 0.9921119875569381\n",
      "7000 Action 1, p [0.00246081 0.99261756 0.00246081 0.00246081] -> Avg. R 0.9914297957434652\n",
      "8000 Action 1, p [0.00249357 0.99251928 0.00249357 0.00249357] -> Avg. R 0.991626046744157\n",
      "9000 Action 1, p [0.00253288 0.99240135 0.00253288 0.00253288] -> Avg. R 0.9921119875569381\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "lr = .01\n",
    "beta = 6\n",
    "\n",
    "# -----------------------------------------------\n",
    "R = []\n",
    "P = []\n",
    "\n",
    "r_agent = np.random.rand(num_arms)\n",
    "r_old = r_agent\n",
    "p_action = Distribution()\n",
    "\n",
    "# -----------------------------------------------\n",
    "for n in range(num_episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    # Act!\n",
    "    a, p = policy(r_agent, beta=beta)\n",
    "    p_action.update(a)\n",
    "    _, r, _, _ = env.step(a)\n",
    "\n",
    "    # RW learning.\n",
    "    reward = np.zeros(num_arms)\n",
    "    reward[a] = r\n",
    "\n",
    "    delta = reward - r_agent\n",
    "    r_old = r_agent\n",
    "    r_agent = learn(r_agent, delta, lr)\n",
    "    \n",
    "    # Log \n",
    "    R.append(r)\n",
    "    P.append(p)\n",
    "    \n",
    "    if n % 1000 == 0:\n",
    "        print(f\"{n} Action {a}, p {p} -> Avg. R {np.mean(R)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 0], [0.992, 0.0031, 0.0031, 0.0018])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 0], [0.992, 0.0031, 0.0031, 0.0018])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_action.keys(), p_action.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: a 1, r 1, p_r 0.6666666666666666 -> H 0.6365141682948128\n",
      "1000: a 1, r 1, p_r 0.2622133599202393 -> H 0.5753593141339879\n",
      "2000: a 2, r 0, p_r 0.2556165751372941 -> H 0.5684218751893065\n",
      "0: a 1, r 1, p_r 0.6666666666666666 -> H 0.6365141682948128\n",
      "1000: a 1, r 1, p_r 0.2622133599202393 -> H 0.5753593141339879\n",
      "2000: a 2, r 0, p_r 0.2556165751372941 -> H 0.5684218751893065\n",
      "3000: a 0, r 0, p_r 0.2524142524142524 -> H 0.5649719621848507\n",
      "4000: a 1, r 1, p_r 0.2533100174868848 -> H 0.5659424392334094\n",
      "5000: a 0, r 0, p_r 0.25344793124125525 -> H 0.5660914786901137\n",
      "3000: a 0, r 0, p_r 0.2524142524142524 -> H 0.5649719621848507\n",
      "4000: a 1, r 1, p_r 0.2533100174868848 -> H 0.5659424392334094\n",
      "5000: a 0, r 0, p_r 0.25344793124125525 -> H 0.5660914786901137\n",
      "6000: a 1, r 1, p_r 0.25170747959353656 -> H 0.5642032398052755\n",
      "7000: a 1, r 1, p_r 0.2527488219334571 -> H 0.5653349337013692\n",
      "8000: a 3, r 0, p_r 0.25228039485193055 -> H 0.5648265751846111\n",
      "6000: a 1, r 1, p_r 0.25170747959353656 -> H 0.5642032398052755\n",
      "7000: a 1, r 1, p_r 0.2527488219334571 -> H 0.5653349337013692\n",
      "8000: a 3, r 0, p_r 0.25228039485193055 -> H 0.5648265751846111\n",
      "9000: a 0, r 0, p_r 0.2518049539042541 -> H 0.5643094154155682\n",
      "9000: a 0, r 0, p_r 0.2518049539042541 -> H 0.5643094154155682\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "lr = 0.01\n",
    "beta = 6\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "class Actor(object):\n",
    "    \"\"\"A max H(p(r=1)) actor agent.\"\"\"\n",
    "    def __init__(self, num_arms=4, lr=0.1, beta=4):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.num_arms = num_arms\n",
    "        self.W = np.ones(num_arms) / num_arms\n",
    "        \n",
    "    def __call__(self):\n",
    "        return policy(self.W, beta=self.beta)\n",
    "    \n",
    "    def learn(self, a, p_reward):\n",
    "        self.W[a] += self.lr * (entropy([p_reward, 1 - p_reward]) - np.log(2))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "h_actor = Actor(num_arms=num_arms, lr=lr, beta=beta)\n",
    "h_critic = Distribution()\n",
    "h_critic.update(0)\n",
    "h_critic.update(1)\n",
    "p_action = Distribution()\n",
    "\n",
    "H = []\n",
    "P = []\n",
    "for n in range(num_episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    # Act\n",
    "    a, p = h_actor()\n",
    "    p_action.update(a)\n",
    "    _, r, _, _ = env.step(a)\n",
    "    \n",
    "    # Entopy grad. learning\n",
    "    h_critic.update(r)\n",
    "    p_reward = h_critic(1)\n",
    "    h_actor.learn(a, p_reward)\n",
    "    \n",
    "    # Log\n",
    "    h = entropy([p_reward, 1 - p_reward])\n",
    "    H.append(h)\n",
    "    P.append(p)\n",
    "    \n",
    "    if n % 1000 == 0:    \n",
    "        print(f\"{n}: a {a}, r {r}, p_r {p_reward} -> H {h}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 0, 3, 2], [0.2511, 0.2501, 0.2491, 0.2497])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "([1, 0, 3, 2], [0.2511, 0.2501, 0.2491, 0.2497])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_action.keys(), p_action.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max information value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
