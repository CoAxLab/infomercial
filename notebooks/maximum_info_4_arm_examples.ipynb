{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from scipy.stats import entropy\n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from infomercial.util import Distribution\n",
    "from infomercial.discrete.value import information_value\n",
    "from infomercial.discrete.value import entropy\n",
    "from infomercial.discrete.value import estimate_prob\n",
    "\n",
    "from infomercial.local_gym import BanditFourArmedDeterministicFixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a 4 armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_arms = 4\n",
    "env = BanditFourArmedDeterministicFixed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some util funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, beta=1):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x*beta) / np.sum(np.exp(x*beta), axis=0)\n",
    "\n",
    "def policy(x, beta=1):\n",
    "    \"\"\"Softmax action selection policy\"\"\"\n",
    "    p = softmax(x, beta=beta)\n",
    "    actions = np.arange(0, len(x))\n",
    "    return np.random.choice(actions, p=p), p\n",
    "    \n",
    "def learn(agent, delta, lr=0.1):\n",
    "    agent += lr * delta\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Action 3, p [0.01218356 0.02312752 0.04387168 0.92081725] -> Avg. R 0.0\n",
      "1000 Action 1, p [0.00254124 0.99237566 0.00254138 0.00254171] -> Avg. R 0.8091908091908092\n",
      "2000 Action 1, p [0.00254502 0.99236493 0.00254502 0.00254502] -> Avg. R 0.9020489755122438\n",
      "3000 Action 1, p [0.00251266 0.99246203 0.00251266 0.00251266] -> Avg. R 0.931356214595135\n",
      "0 Action 3, p [0.01218356 0.02312752 0.04387168 0.92081725] -> Avg. R 0.0\n",
      "1000 Action 1, p [0.00254124 0.99237566 0.00254138 0.00254171] -> Avg. R 0.8091908091908092\n",
      "2000 Action 1, p [0.00254502 0.99236493 0.00254502 0.00254502] -> Avg. R 0.9020489755122438\n",
      "3000 Action 1, p [0.00251266 0.99246203 0.00251266 0.00251266] -> Avg. R 0.931356214595135\n",
      "0 Action 3, p [0.01218356 0.02312752 0.04387168 0.92081725] -> Avg. R 0.0\n",
      "1000 Action 1, p [0.00254124 0.99237566 0.00254138 0.00254171] -> Avg. R 0.8091908091908092\n",
      "2000 Action 1, p [0.00254502 0.99236493 0.00254502 0.00254502] -> Avg. R 0.9020489755122438\n",
      "3000 Action 1, p [0.00251266 0.99246203 0.00251266 0.00251266] -> Avg. R 0.931356214595135\n",
      "4000 Action 1, p [0.00264861 0.99205417 0.00264861 0.00264861] -> Avg. R 0.9460134966258436\n",
      "5000 Action 1, p [0.00261637 0.99215088 0.00261637 0.00261637] -> Avg. R 0.9556088782243551\n",
      "6000 Action 1, p [0.00283738 0.99148787 0.00283738 0.00283738] -> Avg. R 0.9615064155974005\n",
      "7000 Action 1, p [0.00259394 0.99221819 0.00259394 0.00259394] -> Avg. R 0.965576346236252\n",
      "8000 Action 1, p [0.00250007 0.99249978 0.00250007 0.00250007] -> Avg. R 0.9692538432695913\n",
      "9000 Action 1, p [0.00250898 0.99247305 0.00250898 0.00250898] -> Avg. R 0.9715587156982558\n",
      "4000 Action 1, p [0.00264861 0.99205417 0.00264861 0.00264861] -> Avg. R 0.9460134966258436\n",
      "5000 Action 1, p [0.00261637 0.99215088 0.00261637 0.00261637] -> Avg. R 0.9556088782243551\n",
      "6000 Action 1, p [0.00283738 0.99148787 0.00283738 0.00283738] -> Avg. R 0.9615064155974005\n",
      "7000 Action 1, p [0.00259394 0.99221819 0.00259394 0.00259394] -> Avg. R 0.965576346236252\n",
      "8000 Action 1, p [0.00250007 0.99249978 0.00250007 0.00250007] -> Avg. R 0.9692538432695913\n",
      "9000 Action 1, p [0.00250898 0.99247305 0.00250898 0.00250898] -> Avg. R 0.9715587156982558\n",
      "4000 Action 1, p [0.00264861 0.99205417 0.00264861 0.00264861] -> Avg. R 0.9460134966258436\n",
      "5000 Action 1, p [0.00261637 0.99215088 0.00261637 0.00261637] -> Avg. R 0.9556088782243551\n",
      "6000 Action 1, p [0.00283738 0.99148787 0.00283738 0.00283738] -> Avg. R 0.9615064155974005\n",
      "7000 Action 1, p [0.00259394 0.99221819 0.00259394 0.00259394] -> Avg. R 0.965576346236252\n",
      "8000 Action 1, p [0.00250007 0.99249978 0.00250007 0.00250007] -> Avg. R 0.9692538432695913\n",
      "9000 Action 1, p [0.00250898 0.99247305 0.00250898 0.00250898] -> Avg. R 0.9715587156982558\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000\n",
    "lr = .01\n",
    "beta = 6\n",
    "\n",
    "# -----------------------------------------------\n",
    "R = []\n",
    "P = []\n",
    "\n",
    "r_agent = np.random.rand(num_arms)\n",
    "r_old = r_agent\n",
    "actions = []\n",
    "\n",
    "# -----------------------------------------------\n",
    "for n in range(num_episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    # Act!\n",
    "    a, p = policy(r_agent, beta=beta)\n",
    "    actions.append(a)\n",
    "    _, r, _, _ = env.step(a)\n",
    "\n",
    "    # RW learning.\n",
    "    reward = np.zeros(num_arms)\n",
    "    reward[a] = r\n",
    "\n",
    "    delta = reward - r_agent\n",
    "    r_old = r_agent\n",
    "    r_agent = learn(r_agent, delta, lr)\n",
    "    \n",
    "    # Log \n",
    "    R.append(r)\n",
    "    P.append(p)\n",
    "    \n",
    "    if n % 1000 == 0:\n",
    "        print(f\"{n} Action {a}, p {p} -> Avg. R {np.mean(R)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3] [0.0051 0.9732 0.0066 0.0151]\n",
      "0.14980979257720722\n",
      "[0, 1, 2, 3] [0.0051 0.9732 0.0066 0.0151]\n",
      "0.14980979257720722\n",
      "[0, 1, 2, 3] [0.0051 0.9732 0.0066 0.0151]\n",
      "0.14980979257720722\n"
     ]
    }
   ],
   "source": [
    "probs, cond = estimate_prob(actions)\n",
    "print(cond, probs)\n",
    "\n",
    "h = entropy(actions)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "lr = 0.01\n",
    "beta = 6\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "class Actor(object):\n",
    "    \"\"\"A max H(p(r=1)) actor agent.\"\"\"\n",
    "    def __init__(self, num_arms=4, lr=0.1, beta=4):\n",
    "        self.lr = lr\n",
    "        self.beta = beta\n",
    "        self.num_arms = num_arms\n",
    "        self.W = np.ones(num_arms) / num_arms\n",
    "        \n",
    "    def __call__(self):\n",
    "        return policy(self.W, beta=self.beta)\n",
    "    \n",
    "    def learn(self, a, p_reward):\n",
    "        self.W[a] += self.lr * (entropy([p_reward, 1 - p_reward]) - np.log(2))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "h_actor = Actor(num_arms=num_arms, lr=lr, beta=beta)\n",
    "h_critic = Distribution()\n",
    "h_critic.update(0)\n",
    "h_critic.update(1)\n",
    "p_action = Distribution()\n",
    "\n",
    "H = []\n",
    "P = []\n",
    "for n in range(num_episodes):\n",
    "    env.reset()\n",
    "    \n",
    "    # Act\n",
    "    a, p = h_actor()\n",
    "    p_action.update(a)\n",
    "    _, r, _, _ = env.step(a)\n",
    "    \n",
    "    # Entopy grad. learning\n",
    "    h_critic.update(r)\n",
    "    p_reward = h_critic(1)\n",
    "    h_actor.learn(a, p_reward)\n",
    "    \n",
    "    # Log\n",
    "    h = entropy([p_reward, 1 - p_reward])\n",
    "    H.append(h)\n",
    "    P.append(p)\n",
    "    \n",
    "    if n % 1000 == 0:    \n",
    "        print(f\"{n}: a {a}, r {r}, p_r {p_reward} -> H {h}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_action.keys(), p_action.probs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max information value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
